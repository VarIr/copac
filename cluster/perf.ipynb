{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import clock\n",
    "from timeit import timeit\n",
    "import memory_profiler\n",
    "%load_ext memory_profiler\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import plot, scatter\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import adjusted_mutual_info_score, accuracy_score, v_measure_score, normalized_mutual_info_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "COPAC: Correlation Partition Clustering\n",
    "\"\"\"\n",
    "\n",
    "# Author: Roman Feldbauer <roman.feldbauer@ofai.at>\n",
    "#         ... <>\n",
    "#         ... <>\n",
    "#         ... <>\n",
    "#\n",
    "# License: ...\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg as LA\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from sklearn.cluster.dbscan_ import dbscan\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "def _cdist(P, Q, Mhat_P):\n",
    "    \"\"\" Correlation distance between P and Q (not symmetric).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The squareroot of cdist is taken later. The advantage here is to\n",
    "    save some computation, as we can first take the maximum of\n",
    "    two cdists, and then take the root of the 'winner' only.\n",
    "    \"\"\"\n",
    "    PQ_diff = P[np.newaxis, :] - Q\n",
    "    return (PQ_diff @ Mhat_P * PQ_diff).sum(axis=1)\n",
    "\n",
    "\n",
    "def copac(X, k=10, mu=5, eps=0.5, alpha=0.85, metric='euclidean',\n",
    "          metric_params=None, algorithm='auto', leaf_size=30, p=None,\n",
    "          n_jobs=1, sample_weight=None):\n",
    "    \"\"\"Perform COPAC clustering from vector array.\n",
    "    Read more in the :ref:`User Guide <copac>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array of shape (n_samples, n_features)\n",
    "        A feature array.\n",
    "    k : int, optional, default=10\n",
    "        Size of local neighborhood for local correlation dimensionality.\n",
    "        The paper suggests k >= 3 * n_features.\n",
    "    mu : int, optional, default=5\n",
    "        Minimum number of points in a cluster with mu <= k.\n",
    "    eps : float, optional, default=0.5\n",
    "        Neighborhood predicate, so that neighbors are closer than `eps`.\n",
    "    alpha : float in ]0,1[, optional, default=0.85\n",
    "        Threshold of how much variance needs to be explained by Eigenvalues.\n",
    "        Assumed to be robust in range 0.8 <= alpha <= 0.9 [see Ref.]\n",
    "    metric : string, or callable\n",
    "        The metric to use when calculating distance between instances in a\n",
    "        feature array. If metric is a string or callable, it must be one of\n",
    "        the options allowed by sklearn.metrics.pairwise.pairwise_distances\n",
    "        for its metric parameter.\n",
    "        If metric is \"precomputed\", `X` is assumed to be a distance matrix and\n",
    "        must be square.\n",
    "    metric_params : dict, optional\n",
    "        Additional keyword arguments for the metric function.\n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
    "        The algorithm to be used by the scikit-learn NearestNeighbors module\n",
    "        to compute pointwise distances and find nearest neighbors.\n",
    "        See NearestNeighbors module documentation for details.\n",
    "    leaf_size : int, optional (default = 30)\n",
    "        Leaf size passed to BallTree or cKDTree. This can affect the speed\n",
    "        of the construction and query, as well as the memory required\n",
    "        to store the tree. The optimal value depends\n",
    "        on the nature of the problem.\n",
    "    p : float, optional\n",
    "        The power of the Minkowski metric to be used to calculate distance\n",
    "        between points.\n",
    "    n_jobs : int, optional, default=1\n",
    "        Number of parallel processes. Use all cores with n_jobs=-1.\n",
    "    sample_weight : None\n",
    "        Currently ignored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : array [n_samples]\n",
    "        Cluster labels for each point. Noisy samples are given the label -1.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Elke Achtert, Christian Bohm, Hans-Peter Kriegel, Peer Kroger,\n",
    "    A. Z. (n.d.). Robust, complete, and efficient correlation\n",
    "    clustering. In Proceedings of the Seventh SIAM International\n",
    "    Conference on Data Mining, April 26-28, 2007, Minneapolis,\n",
    "    Minnesota, USA (2007), pp. 413â€“418.\n",
    "    \"\"\"\n",
    "    t0 = clock()\n",
    "    t_start = t0\n",
    "    X = check_array(X)\n",
    "    n, d = X.shape\n",
    "    y = -np.ones(n, dtype=np.int)\n",
    "    if n_jobs == -1:\n",
    "        n_jobs = cpu_count()\n",
    "    t1 = clock()\n",
    "    print(f\"init took {t1 - t0:.3f} sec.\")\n",
    "    t0 = clock()\n",
    "    # Calculating M^ just once requires lots of memory...\n",
    "    lambda_ = np.zeros(n, dtype=int)\n",
    "    M_hat = list()\n",
    "\n",
    "    # Get nearest neighbors\n",
    "    nn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm=algorithm,\n",
    "                          leaf_size=leaf_size, metric_params=metric_params,\n",
    "                          p=p, n_jobs=n_jobs)\n",
    "    nn.fit(X)\n",
    "    knns = nn.kneighbors(return_distance=False)\n",
    "    t1 = clock()\n",
    "    print(f\"k-NN took {t1 - t0:.3f} sec.\")\n",
    "    t0 = clock()\n",
    "    for P, knn in enumerate(knns):\n",
    "        N_P = X[knn]\n",
    "\n",
    "        # Corr. cluster cov. matrix\n",
    "        Sigma = np.cov(N_P[:, :], rowvar=False, ddof=0)\n",
    "\n",
    "        # Decompose spsd matrix, and sort Eigenvalues descending\n",
    "        E, V = LA.eigh(Sigma)\n",
    "        del Sigma\n",
    "        E = np.sort(E)[::-1]\n",
    "\n",
    "        # Local correlation dimension\n",
    "        explanation_portion = np.cumsum(E) / E.sum()\n",
    "        lambda_P = np.searchsorted(explanation_portion, alpha, side='left')\n",
    "        lambda_P += 1\n",
    "        lambda_[P] = lambda_P\n",
    "        # Correlation distance matrix\n",
    "        E_hat = (np.arange(1, d + 1) > lambda_P).astype(int)\n",
    "        M_hat.append(V @ np.diag(E_hat) @ V.T)\n",
    "    t1 = clock()\n",
    "    print(f\"corr dim took {t1 - t0:.3f} sec.\")\n",
    "    t0 = clock()\n",
    "    # Group pts by corr. dim.\n",
    "    argsorted = np.argsort(lambda_)\n",
    "    edges, _ = np.histogram(lambda_[argsorted], bins=np.arange(1, d + 2))\n",
    "    Ds = np.split(argsorted, np.cumsum(edges))\n",
    "    # Loop over partitions according to local corr. dim.\n",
    "    max_label = 0\n",
    "    used_y = np.zeros_like(y, dtype=bool)\n",
    "    t1 = clock()\n",
    "    print(f\"grouping took {t1 - t0:.3f} sec.\")\n",
    "    t0 = clock()\n",
    "    for D in Ds:\n",
    "        n_D = D.shape[0]\n",
    "        cdist_P = -np.ones(n_D * (n_D - 1) // 2, dtype=np.float)\n",
    "        cdist_Q = -np.ones((n_D, n_D), dtype=np.float)\n",
    "        start = 0\n",
    "        t1 = clock()\n",
    "        print(f\"init took {t1 - t0:.3f} sec.\")\n",
    "        t0 = clock()\n",
    "        # Calculate triu part of distance matrix\n",
    "        for i in range(0, n_D - 1):\n",
    "            p = D[i]\n",
    "            # Vectorized inner loop\n",
    "            q = D[i + 1:n_D]\n",
    "            stop = start + n_D - i - 1\n",
    "            cdist_P[start:stop] = _cdist(X[p], X[q], M_hat[p])\n",
    "            start = stop\n",
    "        t1 = clock()\n",
    "        print(f\"triu took {t1 - t0:.3f} sec.\")\n",
    "        t0 = clock()\n",
    "        # Calculate tril part of distance matrix\n",
    "        for i in range(1, n_D):\n",
    "            q = D[i]\n",
    "            p = D[0:i]\n",
    "            cdist_Q[i, :i] = _cdist(X[q], X[p], M_hat[q])\n",
    "        # Extract tril to 1D array\n",
    "        # TODO simplify...\n",
    "        cdist_Q = cdist_Q.T[np.triu_indices_from(cdist_Q, k=1)]\n",
    "        t1 = clock()\n",
    "        print(f\"tril took {t1 - t0:.3f} sec.\")\n",
    "        t0 = clock()\n",
    "        cdist = np.block([[cdist_P], [cdist_Q]])\n",
    "        # Square root of the higher value of cdist_P, cdist_Q\n",
    "        cdist = np.sqrt(cdist.max(axis=0))\n",
    "        t1 = clock()\n",
    "        print(f\"... took {t1 - t0:.3f} sec.\")\n",
    "        t0 = clock()\n",
    "        # Perform DBSCAN with full distance matrix\n",
    "        cdist = squareform(cdist)\n",
    "        clust = dbscan(X=cdist, eps=eps, min_samples=mu,\n",
    "                       metric='precomputed', n_jobs=n_jobs)\n",
    "        _, labels = clust\n",
    "        t1 = clock()\n",
    "        print(f\"dbscan took {t1 - t0:.3f} sec.\")\n",
    "        t0 = clock()\n",
    "        # Each DBSCAN run is unaware of previous ones,\n",
    "        # so we need to keep track of previous cluster IDs\n",
    "        y_D = labels + max_label\n",
    "        new_labels = np.unique(labels[labels >= 0]).size\n",
    "        max_label += new_labels\n",
    "        # TODO check correct indexing of label array `y`\n",
    "        y[D] = y_D\n",
    "        used_y[D] = True\n",
    "        t1 = clock()\n",
    "        print(f\"rest took {t1 - t0:.3f} sec.\")\n",
    "        t0 = clock()\n",
    "    t1 = clock()\n",
    "    print(f\"corr dist matrices took {t1 - t0:.3f} sec.\")\n",
    "    t0 = clock()\n",
    "    assert np.all(used_y), \"Not all samples got labels!\"\n",
    "    t_stop = t0\n",
    "    print(f\"Total: {t_stop - t_start:.3f} sec.\")\n",
    "    return y\n",
    "\n",
    "\n",
    "class COPAC(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"Perform COPAC clustering from vector array.\n",
    "    Read more in the :ref:`User Guide <copac>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int, optional, default=10\n",
    "        Size of local neighborhood for local correlation dimensionality.\n",
    "        The paper suggests k >= 3 * n_features.\n",
    "    mu : int, optional, default=5\n",
    "        Minimum number of points in a cluster with mu <= k.\n",
    "    eps : float, optional, default=0.5\n",
    "        Neighborhood predicate, so that neighbors are closer than `eps`.\n",
    "    alpha : float in ]0,1[, optional, default=0.85\n",
    "        Threshold of how much variance needs to be explained by Eigenvalues.\n",
    "        Assumed to be robust in range 0.8 <= alpha <= 0.9 [see Ref.]\n",
    "    metric : string, or callable\n",
    "        The metric to use when calculating distance between instances in a\n",
    "        feature array. If metric is a string or callable, it must be one of\n",
    "        the options allowed by sklearn.metrics.pairwise.pairwise_distances\n",
    "        for its metric parameter.\n",
    "        If metric is \"precomputed\", `X` is assumed to be a distance matrix and\n",
    "        must be square.\n",
    "    metric_params : dict, optional\n",
    "        Additional keyword arguments for the metric function.\n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
    "        The algorithm to be used by the scikit-learn NearestNeighbors module\n",
    "        to compute pointwise distances and find nearest neighbors.\n",
    "        See NearestNeighbors module documentation for details.\n",
    "    leaf_size : int, optional (default = 30)\n",
    "        Leaf size passed to BallTree or cKDTree. This can affect the speed\n",
    "        of the construction and query, as well as the memory required\n",
    "        to store the tree. The optimal value depends\n",
    "        on the nature of the problem.\n",
    "    p : float, optional\n",
    "        The power of the Minkowski metric to be used to calculate distance\n",
    "        between points.\n",
    "    n_jobs : int, optional, default=1\n",
    "        Number of parallel processes. Use all cores with n_jobs=-1.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels_ : array, shape = [n_samples]\n",
    "        Cluster labels for each point in the dataset given to fit().\n",
    "        Noisy samples are given the label -1.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    ...\n",
    "    References\n",
    "    ----------\n",
    "    Elke Achtert, Christian Bohm, Hans-Peter Kriegel, Peer Kroger,\n",
    "    A. Z. (n.d.). Robust, complete, and efficient correlation\n",
    "    clustering. In Proceedings of the Seventh SIAM International\n",
    "    Conference on Data Mining, April 26-28, 2007, Minneapolis,\n",
    "    Minnesota, USA (2007), pp. 413â€“418.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=10, mu=5, eps=0.5, alpha=0.85,\n",
    "                 metric='euclidean', metric_params=None, algorithm='auto',\n",
    "                 leaf_size=30, p=None, n_jobs=1):\n",
    "        self.k = k\n",
    "        self.mu = mu\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.metric = metric\n",
    "        self.metric_params = metric_params\n",
    "        self.algorithm = algorithm\n",
    "        self.leaf_size = leaf_size\n",
    "        self.p = p\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):  # @UnusedVariable pylint: disable=unused-argument\n",
    "        \"\"\"Perform COPAC clustering from features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            A feature array.\n",
    "        sample_weight : array, shape (n_samples,), optional\n",
    "            Weight of each sample, such that a sample with a weight of at least\n",
    "            ``min_samples`` is by itself a core sample; a sample with negative\n",
    "            weight may inhibit its eps-neighbor from being core.\n",
    "            Note that weights are absolute, and default to 1.\n",
    "        y : Ignored\n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        clust = copac(X, sample_weight=sample_weight,\n",
    "                      **self.get_params())\n",
    "        self.labels_ = clust  #pylint: disable=attribute-defined-outside-init\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, y=None, sample_weight=None):  #pylint: disable=arguments-differ\n",
    "        \"\"\"Performs clustering on X and returns cluster labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray matrix of shape (n_samples, n_features)\n",
    "            A feature array.\n",
    "        sample_weight : array, shape (n_samples,), optional\n",
    "            Weight of each sample, such that a sample with a weight of at least\n",
    "            ``min_samples`` is by itself a core sample; a sample with negative\n",
    "            weight may inhibit its eps-neighbor from being core.\n",
    "            Note that weights are absolute, and default to 1.\n",
    "        y : Ignored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            cluster labels\n",
    "        \"\"\"\n",
    "        self.fit(X, sample_weight=sample_weight)\n",
    "        return self.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.9 ms Â± 5.49 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "48.7 ms Â± 1.06 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
      "658 ms Â± 31.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
      "42.1 s Â± 3.85 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "clstr = COPAC()\n",
    "for n in [20, 100, 1000, 10000]:\n",
    "    X = np.random.rand(n, 50)\n",
    "    %timeit clstr.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 347.40 MiB, increment: 0.00 MiB\n",
      "peak memory: 347.40 MiB, increment: 0.00 MiB\n",
      "peak memory: 447.65 MiB, increment: 100.25 MiB\n",
      "peak memory: 636.50 MiB, increment: 289.10 MiB\n",
      "peak memory: 787.20 MiB, increment: 439.80 MiB\n",
      "peak memory: 1155.91 MiB, increment: 802.97 MiB\n",
      "peak memory: 1416.86 MiB, increment: 1063.93 MiB\n",
      "peak memory: 1725.25 MiB, increment: 1372.31 MiB\n",
      "peak memory: 2141.61 MiB, increment: 1788.67 MiB\n",
      "peak memory: 2574.90 MiB, increment: 2221.97 MiB\n"
     ]
    }
   ],
   "source": [
    "clstr = COPAC()\n",
    "for n in range(1, 11):\n",
    "    n = 1000 * n\n",
    "    X = np.random.rand(n, 50)\n",
    "    %memit clstr.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime of (5000, 20) sized matrix: 15 sec.\n",
    "* initial k-NN: 1 sec.\n",
    "* calculate correlation dimension: 4 sec.\n",
    "* correlation distance triu: 4.4 sec.\n",
    "* correlation distance tril: 4.6 sec.\n",
    "* correlation distance rest: 0.7 sec. (incl. DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init took 0.000 sec.\n",
      "k-NN took 0.957 sec.\n",
      "corr dim took 4.134 sec.\n",
      "grouping took 0.001 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.002 sec.\n",
      "triu took 0.076 sec.\n",
      "tril took 0.082 sec.\n",
      "... took 0.005 sec.\n",
      "dbscan took 0.008 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.249 sec.\n",
      "triu took 4.363 sec.\n",
      "tril took 4.568 sec.\n",
      "... took 0.225 sec.\n",
      "dbscan took 0.154 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.003 sec.\n",
      "triu took 0.002 sec.\n",
      "tril took 0.001 sec.\n",
      "... took 0.008 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.000 sec.\n",
      "rest took 0.000 sec.\n",
      "corr dist matrices took 0.000 sec.\n",
      "Total: 14.864 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(5000, 20)\n",
    "clstr.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime of (10000, 20) sized matrix: 49 sec.\n",
    "* initial k-NN: 4 sec.\n",
    "* calculate correlation dimension: 10 sec.\n",
    "* correlation distance triu: 14.6 sec.\n",
    "* correlation distance tril: 18.0 sec.\n",
    "* correlation distance rest: 1.6 sec. (incl. DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init took 0.000 sec.\n",
      "k-NN took 3.784 sec.\n",
      "corr dim took 9.646 sec.\n",
      "grouping took 0.002 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.011 sec.\n",
      "triu took 0.356 sec.\n",
      "tril took 0.382 sec.\n",
      "... took 0.042 sec.\n",
      "dbscan took 0.029 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.789 sec.\n",
      "triu took 14.611 sec.\n",
      "tril took 17.959 sec.\n",
      "... took 0.875 sec.\n",
      "dbscan took 0.628 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.028 sec.\n",
      "triu took 0.014 sec.\n",
      "tril took 0.009 sec.\n",
      "... took 0.096 sec.\n",
      "dbscan took 0.003 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.001 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "init took 0.000 sec.\n",
      "triu took 0.000 sec.\n",
      "tril took 0.000 sec.\n",
      "... took 0.000 sec.\n",
      "dbscan took 0.001 sec.\n",
      "rest took 0.000 sec.\n",
      "corr dist matrices took 0.000 sec.\n",
      "Total: 49.309 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(10000, 20)\n",
    "clstr.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasonable performance. Most time spent in correlation distance, corr. dimension, k-NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
